{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuu11VulauyfXKuyjPIDq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamikaze02/COSC-4P96-ASSIGNMENT-1/blob/main/COSC_4P96_ASS1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhwQEuGiaaIP",
        "outputId": "14ad3e18-cfb4-41bc-fc73-5ca2c220f073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(38400, 28, 28)\n",
            "Augumented:  [[[1.03505552]\n",
            "  [2.06085825]\n",
            "  [2.99126029]\n",
            "  [3.97349644]]\n",
            "\n",
            " [[5.02562284]\n",
            "  [6.0072484]\n",
            "  [6.95549059]\n",
            "  [7.95023346]]\n",
            "\n",
            " [[8.99178696]\n",
            "  [10.0287008]\n",
            "  [10.9237118]\n",
            "  [11.9978142]]\n",
            "\n",
            " [[12.9670982]\n",
            "  [14.0339336]\n",
            "  [15.0236034]\n",
            "  [15.9756308]]]\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocessWithMinMax(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  return (image/255.0)\n",
        "\n",
        "def preprocessWithZScore(image):\n",
        "\n",
        "  data = tf.cast(image, tf.float32)\n",
        "  mean= tf.reduce_mean(data)\n",
        "  std = tf.math.reduce_std(data)\n",
        "\n",
        "  zScore=(data-mean)/std\n",
        "  return zScore\n",
        "\n",
        "def dataAugumentationWithRandomCrop(image):\n",
        "  data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomCrop(24, 24),        # crop to 24x24\n",
        "    tf.keras.layers.Resizing(28, 28),          # resize back to 28x28\n",
        "    tf.keras.layers.RandomTranslation(0.1, 0.1) # translate up to 10% of height/width\n",
        "])\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = data_augmentation(image, training=True)\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def dataAugumentationWithRandomNoise(image):\n",
        "    image = tf.cast(image, tf.float32) # normalize\n",
        "\n",
        "    noise = tf.random.normal(\n",
        "        shape=tf.shape(image),\n",
        "        mean=0.0,\n",
        "        stddev=0.05\n",
        "    )\n",
        "\n",
        "    noisy_image = image + noise\n",
        "\n",
        "    #noisy_image = tf.clip_by_value(noisy_image, 0.0, 1.0)\n",
        "    return noisy_image\n",
        "\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x,y),_=fashion_mnist.load_data()\n",
        "\n",
        "#randomize the indexs\n",
        "indices = np.random.permutation(len(x))\n",
        "\n",
        "split_index = int(0.8 * len(x))\n",
        "\n",
        "\n",
        "# training data 80%\n",
        "trainData=x[indices[:split_index]]\n",
        "trainLabels=y[indices[:split_index]]\n",
        "\n",
        "\n",
        "remainingIndices=indices[split_index:]\n",
        "split_test=int(0.5*len(remainingIndices))\n",
        "#print(split_test)\n",
        "\n",
        "#test data 10%\n",
        "testData=x[indices[:split_test]]\n",
        "testLabel=y[indices[:split_test]]\n",
        "\n",
        "#validation\n",
        "validationData=x[indices[split_test:]]\n",
        "validationLabel=y[indices[split_test:]]\n",
        "\n",
        "# Calculate 20% of train data for label\n",
        "numberLabel=int(0.2*len(trainData))\n",
        "#print(numberLabel)\n",
        "\n",
        "\n",
        "x_labeled = trainData[:numberLabel]\n",
        "y_labeled = trainLabels[:numberLabel]\n",
        "\n",
        "# 80% of train data for unlabel\n",
        "x_unlabeled=trainData[numberLabel:]\n",
        "\n",
        "print(x_unlabeled.shape)\n",
        "\n",
        "# label dataset\n",
        "label_dataset = tf.data.Dataset.from_tensor_slices((x_labeled, y_labeled))\n",
        "\n",
        "#Unlabeled dataset\n",
        "unlabel_dataset = tf.data.Dataset.from_tensor_slices(x_unlabeled)\n",
        "#dataset for data processing with minMax\n",
        "label_dataset1=label_dataset\n",
        "\n",
        "label_dataset1 =label_dataset1.map(lambda x, y: (preprocessWithMinMax(x), y))\n",
        "unlabel_dataset1=unlabel_dataset.map(lambda x: (preprocessWithMinMax(x)))\n",
        "\n",
        "\n",
        "#dataset for data processing with z score\n",
        "label_dataset2=label_dataset\n",
        "label_dataset2 =label_dataset2.map(lambda x, y: (preprocessWithZScore(x), y))\n",
        "unlabel_dataset2=unlabel_dataset.map(lambda x: (preprocessWithZScore(x)))\n",
        "test_image = tf.reshape(tf.range(1, 17, dtype=tf.float32), (4, 4, 1))\n",
        "\n",
        "tf.print(\"Augumented: \", dataAugumentationWithRandomNoise(test_image))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#load data set\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x, y), _ = fashion_mnist.load_data()\n",
        "\n",
        "#Split to 80% training 10% testing 10% validating\n",
        "#Randomize indices\n",
        "#Split training\n",
        "#Find Remaining indices\n",
        "#Split the remaining indices into validating and testing data set\n",
        "indices = np.random.permutation(len(x))\n",
        "\n",
        "split_train = int(0.8 * len(x))\n",
        "train_indices = indices[:split_train]\n",
        "remaining_indices = indices[split_train:]\n",
        "\n",
        "split_test = int(0.5 * len(remaining_indices))\n",
        "test_indices = remaining_indices[:split_test]\n",
        "val_indices = remaining_indices[split_test:]\n",
        "\n",
        "trainData = x[train_indices]\n",
        "trainLabels = y[train_indices]\n",
        "\n",
        "testData = x[test_indices]\n",
        "testLabels = y[test_indices]\n",
        "\n",
        "validationData = x[val_indices]\n",
        "validationLabels = y[val_indices]\n",
        "\n",
        "# 20% LABEL 80% UNLABELED FROM TRAINING DATA SET\n",
        "\n",
        "num_labeled = int(0.2 * len(trainData))\n",
        "\n",
        "x_labeled = trainData[:num_labeled]\n",
        "y_labeled = trainLabels[:num_labeled]\n",
        "\n",
        "x_unlabeled = trainData[num_labeled:]\n",
        "\n",
        "\n",
        "######################################\n",
        "#DATA PREPARATION PART\n",
        "######################################\n",
        "\n",
        "global_mean = np.mean(trainData)\n",
        "global_std = np.std(trainData)\n",
        "\n",
        "def preprocess_minmax(image):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = image[..., tf.newaxis]  # Add channel dimension\n",
        "    return image\n",
        "def preprocess_zscore(image):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image - global_mean) / global_std\n",
        "    image = image[..., tf.newaxis]\n",
        "    return image\n",
        "####################################\n",
        "#DATA ARGUMENTATION\n",
        "###################################\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomFlip(\"vertical\")\n",
        "])\n",
        "\n",
        "def add_gaussian_noise(image):\n",
        "    noise = tf.random.normal(\n",
        "        shape=tf.shape(image),\n",
        "        mean=0.0,\n",
        "        stddev=0.05\n",
        "    )\n",
        "    image = image + noise\n",
        "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "    return image\n",
        "\n",
        "def augment(image):\n",
        "    image = data_augmentation(image, training=True)\n",
        "    return image"
      ],
      "metadata": {
        "id": "V0e--kYl4dKh"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}